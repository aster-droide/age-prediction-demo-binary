{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65606dc-4e18-434c-be31-33e1b50ca669",
   "metadata": {},
   "source": [
    "# Notebook for generating the Production Demo for Binary Age Prediction of Domestic Felines (kitten, senior)\n",
    "\n",
    "Three cat_ids are selected that each have 7 contributions. \n",
    "\n",
    "Demo samples are removed from training set and model is built on remaining data. \n",
    "\n",
    "Demo samples are available for the production demo in https://github.com/aster-droide/age-prediction-demo-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8642f315-93d3-4406-8f4a-0d2527dfe2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Imbalanced-learn import\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# TensorFlow and Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax, AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.regularizers import l1, l2, L1L2\n",
    "\n",
    "# Optuna import\n",
    "import optuna\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# to save the scaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397e3b13-5869-4569-94c1-1c662c2d986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senior    306\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(5390) \n",
    "np.random.seed(5390)\n",
    "tf.random.set_seed(5390)\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 10),\n",
    "    'senior': (10, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "# Drop Adult\n",
    "dataframe.drop(dataframe[dataframe['age_group'] == 'adult'].index, inplace=True)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f7e4f-740d-4b02-b607-a1250fe976e2",
   "metadata": {},
   "source": [
    "# save demo rows to external csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d81d073-1297-4234-9573-d47d6e04cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows corresponding to the specified cat_id values\n",
    "selected_cat_ids = ['117A', '050A']\n",
    "demo_samples = dataframe[dataframe['cat_id'].isin(selected_cat_ids)]\n",
    "\n",
    "# Save the selected samples to a CSV file\n",
    "demo_samples.to_csv('demo_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12a12fcd-e5fc-4ab4-a13d-949a94708d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>gender</th>\n",
       "      <th>target</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>3426.3960</td>\n",
       "      <td>-1453.4320</td>\n",
       "      <td>-3635.5828</td>\n",
       "      <td>178.96103</td>\n",
       "      <td>306.443570</td>\n",
       "      <td>5830.9550</td>\n",
       "      <td>-2044.4401</td>\n",
       "      <td>-2213.7430</td>\n",
       "      <td>7086.1597</td>\n",
       "      <td>2355.9998</td>\n",
       "      <td>...</td>\n",
       "      <td>3041.8423</td>\n",
       "      <td>-527.45470</td>\n",
       "      <td>-3442.1597</td>\n",
       "      <td>4118.9020</td>\n",
       "      <td>-7263.3790</td>\n",
       "      <td>-1356.3002</td>\n",
       "      <td>F</td>\n",
       "      <td>18.0</td>\n",
       "      <td>117A</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>3355.9521</td>\n",
       "      <td>-1303.4120</td>\n",
       "      <td>-3631.6050</td>\n",
       "      <td>141.45258</td>\n",
       "      <td>244.160480</td>\n",
       "      <td>5786.3228</td>\n",
       "      <td>-2031.5255</td>\n",
       "      <td>-2233.3796</td>\n",
       "      <td>7078.1445</td>\n",
       "      <td>2368.2544</td>\n",
       "      <td>...</td>\n",
       "      <td>3040.0510</td>\n",
       "      <td>-640.22815</td>\n",
       "      <td>-3344.3310</td>\n",
       "      <td>3989.8276</td>\n",
       "      <td>-7187.9310</td>\n",
       "      <td>-1249.6372</td>\n",
       "      <td>X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>050A</td>\n",
       "      <td>kitten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2545.7540</td>\n",
       "      <td>-1022.4693</td>\n",
       "      <td>-2688.3418</td>\n",
       "      <td>83.26001</td>\n",
       "      <td>181.085170</td>\n",
       "      <td>4263.4214</td>\n",
       "      <td>-1531.0897</td>\n",
       "      <td>-1716.8507</td>\n",
       "      <td>5238.8730</td>\n",
       "      <td>1717.6139</td>\n",
       "      <td>...</td>\n",
       "      <td>2250.8567</td>\n",
       "      <td>-462.13022</td>\n",
       "      <td>-2463.6191</td>\n",
       "      <td>2999.9104</td>\n",
       "      <td>-5323.4185</td>\n",
       "      <td>-910.9742</td>\n",
       "      <td>X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>050A</td>\n",
       "      <td>kitten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2886.9104</td>\n",
       "      <td>-1155.1565</td>\n",
       "      <td>-3081.1362</td>\n",
       "      <td>104.03778</td>\n",
       "      <td>181.506550</td>\n",
       "      <td>4943.7730</td>\n",
       "      <td>-1759.0626</td>\n",
       "      <td>-1962.9388</td>\n",
       "      <td>6099.0923</td>\n",
       "      <td>1992.6163</td>\n",
       "      <td>...</td>\n",
       "      <td>2565.9219</td>\n",
       "      <td>-504.01306</td>\n",
       "      <td>-2830.1445</td>\n",
       "      <td>3407.9817</td>\n",
       "      <td>-6132.1660</td>\n",
       "      <td>-1061.8490</td>\n",
       "      <td>X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>050A</td>\n",
       "      <td>kitten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>3212.6990</td>\n",
       "      <td>-1365.4951</td>\n",
       "      <td>-3333.8115</td>\n",
       "      <td>174.96430</td>\n",
       "      <td>375.178700</td>\n",
       "      <td>5477.5283</td>\n",
       "      <td>-1932.7018</td>\n",
       "      <td>-2047.5621</td>\n",
       "      <td>6611.2944</td>\n",
       "      <td>2187.4224</td>\n",
       "      <td>...</td>\n",
       "      <td>2755.9788</td>\n",
       "      <td>-451.63200</td>\n",
       "      <td>-3125.1729</td>\n",
       "      <td>3826.0570</td>\n",
       "      <td>-6694.3270</td>\n",
       "      <td>-1277.1831</td>\n",
       "      <td>F</td>\n",
       "      <td>18.0</td>\n",
       "      <td>117A</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>3224.6812</td>\n",
       "      <td>-1306.4841</td>\n",
       "      <td>-3483.2856</td>\n",
       "      <td>155.92056</td>\n",
       "      <td>214.709520</td>\n",
       "      <td>5574.1610</td>\n",
       "      <td>-1957.7963</td>\n",
       "      <td>-2125.3423</td>\n",
       "      <td>6849.4150</td>\n",
       "      <td>2297.2240</td>\n",
       "      <td>...</td>\n",
       "      <td>2919.4226</td>\n",
       "      <td>-584.55370</td>\n",
       "      <td>-3240.3708</td>\n",
       "      <td>3825.4539</td>\n",
       "      <td>-6930.2627</td>\n",
       "      <td>-1185.7925</td>\n",
       "      <td>X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>050A</td>\n",
       "      <td>kitten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>2827.5178</td>\n",
       "      <td>-1220.6775</td>\n",
       "      <td>-2959.4004</td>\n",
       "      <td>150.27759</td>\n",
       "      <td>115.613266</td>\n",
       "      <td>4793.8667</td>\n",
       "      <td>-1631.2704</td>\n",
       "      <td>-1904.5444</td>\n",
       "      <td>5927.4536</td>\n",
       "      <td>1959.3539</td>\n",
       "      <td>...</td>\n",
       "      <td>2474.9430</td>\n",
       "      <td>-398.57608</td>\n",
       "      <td>-2907.0308</td>\n",
       "      <td>3327.1280</td>\n",
       "      <td>-5999.8540</td>\n",
       "      <td>-1065.6439</td>\n",
       "      <td>F</td>\n",
       "      <td>18.0</td>\n",
       "      <td>117A</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>3376.4229</td>\n",
       "      <td>-1375.8116</td>\n",
       "      <td>-3668.7173</td>\n",
       "      <td>135.95068</td>\n",
       "      <td>188.348180</td>\n",
       "      <td>5808.9460</td>\n",
       "      <td>-2060.1282</td>\n",
       "      <td>-2271.3206</td>\n",
       "      <td>7147.0750</td>\n",
       "      <td>2407.6265</td>\n",
       "      <td>...</td>\n",
       "      <td>3057.3933</td>\n",
       "      <td>-641.95850</td>\n",
       "      <td>-3405.4680</td>\n",
       "      <td>4017.4436</td>\n",
       "      <td>-7231.2450</td>\n",
       "      <td>-1256.0560</td>\n",
       "      <td>X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>050A</td>\n",
       "      <td>kitten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>3467.8066</td>\n",
       "      <td>-1463.3040</td>\n",
       "      <td>-3717.7402</td>\n",
       "      <td>158.17169</td>\n",
       "      <td>243.690380</td>\n",
       "      <td>5969.6570</td>\n",
       "      <td>-2132.1025</td>\n",
       "      <td>-2287.6530</td>\n",
       "      <td>7308.6665</td>\n",
       "      <td>2475.6670</td>\n",
       "      <td>...</td>\n",
       "      <td>3129.4420</td>\n",
       "      <td>-651.19727</td>\n",
       "      <td>-3503.1690</td>\n",
       "      <td>4131.7180</td>\n",
       "      <td>-7379.8380</td>\n",
       "      <td>-1321.2385</td>\n",
       "      <td>X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>050A</td>\n",
       "      <td>kitten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>3594.0679</td>\n",
       "      <td>-1586.2054</td>\n",
       "      <td>-3861.7378</td>\n",
       "      <td>175.72461</td>\n",
       "      <td>264.818360</td>\n",
       "      <td>6251.3560</td>\n",
       "      <td>-2162.9236</td>\n",
       "      <td>-2404.4595</td>\n",
       "      <td>7608.1875</td>\n",
       "      <td>2509.8042</td>\n",
       "      <td>...</td>\n",
       "      <td>3204.8180</td>\n",
       "      <td>-628.90150</td>\n",
       "      <td>-3751.4497</td>\n",
       "      <td>4238.1270</td>\n",
       "      <td>-7670.5520</td>\n",
       "      <td>-1459.2391</td>\n",
       "      <td>F</td>\n",
       "      <td>18.0</td>\n",
       "      <td>117A</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>2724.4023</td>\n",
       "      <td>-1170.2838</td>\n",
       "      <td>-2891.0864</td>\n",
       "      <td>167.85834</td>\n",
       "      <td>187.265950</td>\n",
       "      <td>4701.3130</td>\n",
       "      <td>-1625.9728</td>\n",
       "      <td>-1845.0255</td>\n",
       "      <td>5771.5034</td>\n",
       "      <td>1907.2744</td>\n",
       "      <td>...</td>\n",
       "      <td>2410.4739</td>\n",
       "      <td>-412.49857</td>\n",
       "      <td>-2825.3042</td>\n",
       "      <td>3211.5217</td>\n",
       "      <td>-5835.4560</td>\n",
       "      <td>-1092.0288</td>\n",
       "      <td>F</td>\n",
       "      <td>18.0</td>\n",
       "      <td>117A</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>3948.5010</td>\n",
       "      <td>-1684.8629</td>\n",
       "      <td>-4216.7217</td>\n",
       "      <td>153.87580</td>\n",
       "      <td>346.356630</td>\n",
       "      <td>6845.1934</td>\n",
       "      <td>-2399.5598</td>\n",
       "      <td>-2624.3125</td>\n",
       "      <td>8359.9240</td>\n",
       "      <td>2746.5405</td>\n",
       "      <td>...</td>\n",
       "      <td>3535.7515</td>\n",
       "      <td>-715.20844</td>\n",
       "      <td>-4063.1138</td>\n",
       "      <td>4694.6406</td>\n",
       "      <td>-8536.4810</td>\n",
       "      <td>-1616.1896</td>\n",
       "      <td>F</td>\n",
       "      <td>18.0</td>\n",
       "      <td>117A</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>3591.7134</td>\n",
       "      <td>-1448.4086</td>\n",
       "      <td>-3889.8455</td>\n",
       "      <td>143.43640</td>\n",
       "      <td>239.711930</td>\n",
       "      <td>6167.7046</td>\n",
       "      <td>-2170.0356</td>\n",
       "      <td>-2367.3960</td>\n",
       "      <td>7546.9834</td>\n",
       "      <td>2547.8892</td>\n",
       "      <td>...</td>\n",
       "      <td>3244.0760</td>\n",
       "      <td>-663.44670</td>\n",
       "      <td>-3600.0854</td>\n",
       "      <td>4266.6177</td>\n",
       "      <td>-7654.1626</td>\n",
       "      <td>-1332.8375</td>\n",
       "      <td>X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>050A</td>\n",
       "      <td>kitten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>3800.4338</td>\n",
       "      <td>-1666.0050</td>\n",
       "      <td>-4015.5880</td>\n",
       "      <td>187.41885</td>\n",
       "      <td>333.724730</td>\n",
       "      <td>6425.8590</td>\n",
       "      <td>-2310.8684</td>\n",
       "      <td>-2485.6104</td>\n",
       "      <td>7720.6000</td>\n",
       "      <td>2533.7773</td>\n",
       "      <td>...</td>\n",
       "      <td>3238.2612</td>\n",
       "      <td>-609.38130</td>\n",
       "      <td>-3684.1950</td>\n",
       "      <td>4522.4556</td>\n",
       "      <td>-7994.3680</td>\n",
       "      <td>-1598.1201</td>\n",
       "      <td>F</td>\n",
       "      <td>18.0</td>\n",
       "      <td>117A</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3           4          5  \\\n",
       "202  3426.3960 -1453.4320 -3635.5828  178.96103  306.443570  5830.9550   \n",
       "209  3355.9521 -1303.4120 -3631.6050  141.45258  244.160480  5786.3228   \n",
       "210  2545.7540 -1022.4693 -2688.3418   83.26001  181.085170  4263.4214   \n",
       "211  2886.9104 -1155.1565 -3081.1362  104.03778  181.506550  4943.7730   \n",
       "215  3212.6990 -1365.4951 -3333.8115  174.96430  375.178700  5477.5283   \n",
       "239  3224.6812 -1306.4841 -3483.2856  155.92056  214.709520  5574.1610   \n",
       "275  2827.5178 -1220.6775 -2959.4004  150.27759  115.613266  4793.8667   \n",
       "292  3376.4229 -1375.8116 -3668.7173  135.95068  188.348180  5808.9460   \n",
       "339  3467.8066 -1463.3040 -3717.7402  158.17169  243.690380  5969.6570   \n",
       "393  3594.0679 -1586.2054 -3861.7378  175.72461  264.818360  6251.3560   \n",
       "528  2724.4023 -1170.2838 -2891.0864  167.85834  187.265950  4701.3130   \n",
       "551  3948.5010 -1684.8629 -4216.7217  153.87580  346.356630  6845.1934   \n",
       "582  3591.7134 -1448.4086 -3889.8455  143.43640  239.711930  6167.7046   \n",
       "858  3800.4338 -1666.0050 -4015.5880  187.41885  333.724730  6425.8590   \n",
       "\n",
       "             6          7          8          9  ...        122        123  \\\n",
       "202 -2044.4401 -2213.7430  7086.1597  2355.9998  ...  3041.8423 -527.45470   \n",
       "209 -2031.5255 -2233.3796  7078.1445  2368.2544  ...  3040.0510 -640.22815   \n",
       "210 -1531.0897 -1716.8507  5238.8730  1717.6139  ...  2250.8567 -462.13022   \n",
       "211 -1759.0626 -1962.9388  6099.0923  1992.6163  ...  2565.9219 -504.01306   \n",
       "215 -1932.7018 -2047.5621  6611.2944  2187.4224  ...  2755.9788 -451.63200   \n",
       "239 -1957.7963 -2125.3423  6849.4150  2297.2240  ...  2919.4226 -584.55370   \n",
       "275 -1631.2704 -1904.5444  5927.4536  1959.3539  ...  2474.9430 -398.57608   \n",
       "292 -2060.1282 -2271.3206  7147.0750  2407.6265  ...  3057.3933 -641.95850   \n",
       "339 -2132.1025 -2287.6530  7308.6665  2475.6670  ...  3129.4420 -651.19727   \n",
       "393 -2162.9236 -2404.4595  7608.1875  2509.8042  ...  3204.8180 -628.90150   \n",
       "528 -1625.9728 -1845.0255  5771.5034  1907.2744  ...  2410.4739 -412.49857   \n",
       "551 -2399.5598 -2624.3125  8359.9240  2746.5405  ...  3535.7515 -715.20844   \n",
       "582 -2170.0356 -2367.3960  7546.9834  2547.8892  ...  3244.0760 -663.44670   \n",
       "858 -2310.8684 -2485.6104  7720.6000  2533.7773  ...  3238.2612 -609.38130   \n",
       "\n",
       "           124        125        126        127  gender  target  cat_id  \\\n",
       "202 -3442.1597  4118.9020 -7263.3790 -1356.3002       F    18.0    117A   \n",
       "209 -3344.3310  3989.8276 -7187.9310 -1249.6372       X     0.0    050A   \n",
       "210 -2463.6191  2999.9104 -5323.4185  -910.9742       X     0.0    050A   \n",
       "211 -2830.1445  3407.9817 -6132.1660 -1061.8490       X     0.0    050A   \n",
       "215 -3125.1729  3826.0570 -6694.3270 -1277.1831       F    18.0    117A   \n",
       "239 -3240.3708  3825.4539 -6930.2627 -1185.7925       X     0.0    050A   \n",
       "275 -2907.0308  3327.1280 -5999.8540 -1065.6439       F    18.0    117A   \n",
       "292 -3405.4680  4017.4436 -7231.2450 -1256.0560       X     0.0    050A   \n",
       "339 -3503.1690  4131.7180 -7379.8380 -1321.2385       X     0.0    050A   \n",
       "393 -3751.4497  4238.1270 -7670.5520 -1459.2391       F    18.0    117A   \n",
       "528 -2825.3042  3211.5217 -5835.4560 -1092.0288       F    18.0    117A   \n",
       "551 -4063.1138  4694.6406 -8536.4810 -1616.1896       F    18.0    117A   \n",
       "582 -3600.0854  4266.6177 -7654.1626 -1332.8375       X     0.0    050A   \n",
       "858 -3684.1950  4522.4556 -7994.3680 -1598.1201       F    18.0    117A   \n",
       "\n",
       "     age_group  \n",
       "202     senior  \n",
       "209     kitten  \n",
       "210     kitten  \n",
       "211     kitten  \n",
       "215     senior  \n",
       "239     kitten  \n",
       "275     senior  \n",
       "292     kitten  \n",
       "339     kitten  \n",
       "393     senior  \n",
       "528     senior  \n",
       "551     senior  \n",
       "582     kitten  \n",
       "858     senior  \n",
       "\n",
       "[14 rows x 132 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7acdc-98df-4117-8a0e-6819f0beedce",
   "metadata": {},
   "source": [
    "## save embeddings and labels from demo set to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6820eab6-4253-4efd-9faa-8d55fa729cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved demo_sample_0.csv\n",
      "Saved demo_sample_1.csv\n",
      "Saved demo_sample_2.csv\n",
      "Saved demo_sample_3.csv\n",
      "Saved demo_sample_4.csv\n",
      "Saved demo_sample_5.csv\n",
      "Saved demo_sample_6.csv\n",
      "Saved demo_sample_7.csv\n",
      "Saved demo_sample_8.csv\n",
      "Saved demo_sample_9.csv\n",
      "Saved demo_sample_10.csv\n",
      "Saved demo_sample_11.csv\n",
      "Saved demo_sample_12.csv\n",
      "Saved demo_sample_13.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure the target labels are encoded as 0 for kitten and 1 for senior\n",
    "demo_samples = demo_samples.copy()  # Avoid SettingWithCopyWarning\n",
    "demo_samples['label'] = demo_samples['age_group'].apply(lambda x: 0 if x == 'kitten' else 1)\n",
    "\n",
    "# Extract features and labels\n",
    "features = demo_samples.iloc[:, :-5].values\n",
    "labels = demo_samples['label'].values\n",
    "\n",
    "# Save each row to a separate .csv file\n",
    "for i, (feature_row, label) in enumerate(zip(features, labels)):\n",
    "    # Create a DataFrame for the current row\n",
    "    row_df = pd.DataFrame([np.append(feature_row, label)])\n",
    "    \n",
    "    # Create a filename\n",
    "    filename = f'demo_sample_{i}.csv'\n",
    "    \n",
    "    # Save to .csv file\n",
    "    row_df.to_csv(filename, index=False, header=False)\n",
    "    \n",
    "    print(f'Saved {filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e4ca59-8738-41fa-bedf-852b9d87f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined_demo_samples.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure the target labels are encoded as 0 for kitten and 1 for senior\n",
    "demo_samples = demo_samples.copy()  # Avoid SettingWithCopyWarning\n",
    "demo_samples['label'] = demo_samples['age_group'].apply(lambda x: 0 if x == 'kitten' else 1)\n",
    "\n",
    "# Extract features and labels\n",
    "features = demo_samples.iloc[:, :-5].values\n",
    "labels = demo_samples['label'].values\n",
    "\n",
    "# Combine features and labels into a single DataFrame\n",
    "combined_data = np.hstack((features, labels.reshape(-1, 1)))\n",
    "combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "# Create a filename for the combined CSV file\n",
    "combined_filename = 'combined_demo_samples.csv'\n",
    "\n",
    "# Save the combined data to a single CSV file\n",
    "combined_df.to_csv(combined_filename, index=False, header=False)\n",
    "\n",
    "print(f'Saved {combined_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f0bfa11-56c7-4cfc-91b6-e4ae252b452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the demo samples\n",
    "# demo_data = pd.read_csv('/Users/astrid/Documents/Thesis/JupyterNotebooks/April/PRODUCTION-MODEL/demo_samples.csv')\n",
    "\n",
    "# # Extract features (assuming the last four columns are not features)\n",
    "# X_demo = demo_data.iloc[:, :-4].values\n",
    "\n",
    "# # Set numpy print options to print the full array\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# # Print the numpy array\n",
    "# print(X_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32faed01-7376-4bbd-a8e6-c257fccd8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat_id  count age_group\n",
      "0    046A     63    kitten\n",
      "1    103A     33    senior\n",
      "2    047A     28    kitten\n",
      "3    057A     27    senior\n",
      "4    055A     20    senior\n",
      "5    097A     16    senior\n",
      "6    101A     15    senior\n",
      "7    001A     14    senior\n",
      "8    106A     14    senior\n",
      "9    059A     14    senior\n",
      "10   042A     14    kitten\n",
      "11   111A     13    kitten\n",
      "12   028A     13    senior\n",
      "13   039A     12    senior\n",
      "14   116A     12    senior\n",
      "15   051A     12    senior\n",
      "16   025A     11    senior\n",
      "17   016A     10    senior\n",
      "18   014B     10    kitten\n",
      "19   040A     10    kitten\n",
      "20   051B      9    senior\n",
      "21   015A      9    senior\n",
      "22   045A      9    kitten\n",
      "23   094A      8    senior\n",
      "24   117A      7    senior\n",
      "25   050A      7    kitten\n",
      "26   053A      6    senior\n",
      "27   008A      6    senior\n",
      "28   108A      6    senior\n",
      "29   109A      6    kitten\n",
      "30   044A      5    kitten\n",
      "31   025C      5    senior\n",
      "32   104A      4    senior\n",
      "33   056A      3    senior\n",
      "34   058A      3    senior\n",
      "35   113A      3    senior\n",
      "36   060A      3    senior\n",
      "37   011A      2    senior\n",
      "38   054A      2    senior\n",
      "39   093A      2    senior\n",
      "40   061A      2    senior\n",
      "41   090A      1    senior\n",
      "42   110A      1    kitten\n",
      "43   115A      1    kitten\n",
      "44   091A      1    senior\n",
      "45   048A      1    kitten\n",
      "46   043A      1    kitten\n",
      "47   041A      1    kitten\n",
      "48   049A      1    kitten\n",
      "49   024A      1    senior\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each cat_id\n",
    "cat_id_counts = dataframe['cat_id'].value_counts().reset_index()\n",
    "cat_id_counts.columns = ['cat_id', 'count']\n",
    "\n",
    "# Merge with the age group information\n",
    "age_group_info = dataframe[['cat_id', 'age_group']].drop_duplicates()\n",
    "cat_id_counts_with_age_group = cat_id_counts.merge(age_group_info, on='cat_id')\n",
    "\n",
    "# Display the result\n",
    "print(cat_id_counts_with_age_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b769c85b-b581-4525-994d-6be79017b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd022efb-c85e-402e-b431-2c18e6b4e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using StandardScaler\n",
    "scaler_full = StandardScaler().fit(X)\n",
    "X_scaled = scaler_full.transform(X)\n",
    "\n",
    "# Encode labels using one-hot encoding\n",
    "y_encoded = y.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15df483-5d53-4d6f-8fc4-dd5fa0ba2f95",
   "metadata": {},
   "source": [
    "### samples for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82d98458-8fc8-4ce9-83ae-930e56029ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one cat_id for each age group\n",
    "# kitten_cat_id = dataframe[dataframe['age_group'] == 'kitten']['cat_id'].sample(1, random_state=42).iloc[0]\n",
    "# senior_cat_id = dataframe[dataframe['age_group'] == 'senior']['cat_id'].sample(1, random_state=42).iloc[0]\n",
    "\n",
    "kitten_cat_id = \"050A\"\n",
    "senior_cat_id = \"117A\"\n",
    "\n",
    "\n",
    "# Select all rows corresponding to the sampled cat_id values\n",
    "demo_samples = dataframe[(dataframe['cat_id'] == kitten_cat_id) | (dataframe['cat_id'] == senior_cat_id)].index\n",
    "\n",
    "# Convert dataframe indices to positional indices\n",
    "demo_sample_positions = dataframe.index.get_indexer(demo_samples)\n",
    "\n",
    "# Separate demonstration samples using positional indices\n",
    "X_demo = X_scaled[demo_sample_positions]\n",
    "y_demo = y_encoded[demo_sample_positions]\n",
    "\n",
    "# Remove demonstration samples from the training set\n",
    "X_train_full = np.delete(X_scaled, demo_sample_positions, axis=0)\n",
    "y_train_full = np.delete(y_encoded, demo_sample_positions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7545e2b9-1bf9-41f4-a2ce-057b4da5f4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'117A'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senior_cat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c58013-380b-43a4-a28f-3064308cfdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'050A'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kitten_cat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f58d3cf-30be-4f50-9afa-bdfe3b6d4163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([202, 209, 210, 211, 215, 239, 275, 292, 339, 393, 528, 551, 582,\n",
       "            858],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe72ce-7c90-4685-ae78-f8da55aa9024",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "520d9913-55d4-46e1-aecd-9e32e89052fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',  \n",
    "    min_delta=0.001, \n",
    "    patience=30,  \n",
    "    verbose=1,  \n",
    "    restore_best_weights=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d36fca0-3968-4135-a57b-e14d61162ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "29/29 [==============================] - 0s 929us/step - loss: 0.4992 - accuracy: 0.7754\n",
      "Epoch 2/1500\n",
      "29/29 [==============================] - 0s 941us/step - loss: 0.4242 - accuracy: 0.8251\n",
      "Epoch 3/1500\n",
      "29/29 [==============================] - 0s 876us/step - loss: 0.3304 - accuracy: 0.8510\n",
      "Epoch 4/1500\n",
      "29/29 [==============================] - 0s 773us/step - loss: 0.2744 - accuracy: 0.8920\n",
      "Epoch 5/1500\n",
      "29/29 [==============================] - 0s 714us/step - loss: 0.2722 - accuracy: 0.8963\n",
      "Epoch 6/1500\n",
      "29/29 [==============================] - 0s 738us/step - loss: 0.3333 - accuracy: 0.8510\n",
      "Epoch 7/1500\n",
      "29/29 [==============================] - 0s 776us/step - loss: 0.2330 - accuracy: 0.8920\n",
      "Epoch 8/1500\n",
      "29/29 [==============================] - 0s 764us/step - loss: 0.2442 - accuracy: 0.9006\n",
      "Epoch 9/1500\n",
      "29/29 [==============================] - 0s 717us/step - loss: 0.3326 - accuracy: 0.8704\n",
      "Epoch 10/1500\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.2749 - accuracy: 0.8769\n",
      "Epoch 11/1500\n",
      "29/29 [==============================] - 0s 707us/step - loss: 0.1974 - accuracy: 0.9201\n",
      "Epoch 12/1500\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.2033 - accuracy: 0.9179\n",
      "Epoch 13/1500\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.2373 - accuracy: 0.9330\n",
      "Epoch 14/1500\n",
      "29/29 [==============================] - 0s 698us/step - loss: 0.1909 - accuracy: 0.9287\n",
      "Epoch 15/1500\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.1847 - accuracy: 0.9266\n",
      "Epoch 16/1500\n",
      "29/29 [==============================] - 0s 752us/step - loss: 0.1900 - accuracy: 0.9352\n",
      "Epoch 17/1500\n",
      "29/29 [==============================] - 0s 740us/step - loss: 0.1725 - accuracy: 0.9309\n",
      "Epoch 18/1500\n",
      "29/29 [==============================] - 0s 713us/step - loss: 0.1977 - accuracy: 0.9158\n",
      "Epoch 19/1500\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.2247 - accuracy: 0.9136\n",
      "Epoch 20/1500\n",
      "29/29 [==============================] - 0s 820us/step - loss: 0.1546 - accuracy: 0.9417\n",
      "Epoch 21/1500\n",
      "29/29 [==============================] - 0s 876us/step - loss: 0.2014 - accuracy: 0.9093\n",
      "Epoch 22/1500\n",
      "29/29 [==============================] - 0s 761us/step - loss: 0.2037 - accuracy: 0.9158\n",
      "Epoch 23/1500\n",
      "29/29 [==============================] - 0s 751us/step - loss: 0.1456 - accuracy: 0.9525\n",
      "Epoch 24/1500\n",
      "29/29 [==============================] - 0s 702us/step - loss: 0.1910 - accuracy: 0.9158\n",
      "Epoch 25/1500\n",
      "29/29 [==============================] - 0s 725us/step - loss: 0.1654 - accuracy: 0.9330\n",
      "Epoch 26/1500\n",
      "29/29 [==============================] - 0s 763us/step - loss: 0.1764 - accuracy: 0.9374\n",
      "Epoch 27/1500\n",
      "29/29 [==============================] - 0s 713us/step - loss: 0.1633 - accuracy: 0.9330\n",
      "Epoch 28/1500\n",
      "29/29 [==============================] - 0s 713us/step - loss: 0.1314 - accuracy: 0.9590\n",
      "Epoch 29/1500\n",
      "29/29 [==============================] - 0s 648us/step - loss: 0.1474 - accuracy: 0.9417\n",
      "Epoch 30/1500\n",
      "29/29 [==============================] - 0s 685us/step - loss: 0.1637 - accuracy: 0.9417\n",
      "Epoch 31/1500\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.1507 - accuracy: 0.9460\n",
      "Epoch 32/1500\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.1684 - accuracy: 0.9374\n",
      "Epoch 33/1500\n",
      "29/29 [==============================] - 0s 674us/step - loss: 0.1351 - accuracy: 0.9503\n",
      "Epoch 34/1500\n",
      "29/29 [==============================] - 0s 686us/step - loss: 0.1587 - accuracy: 0.9179\n",
      "Epoch 35/1500\n",
      "29/29 [==============================] - 0s 671us/step - loss: 0.1614 - accuracy: 0.9309\n",
      "Epoch 36/1500\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.1702 - accuracy: 0.9330\n",
      "Epoch 37/1500\n",
      "29/29 [==============================] - 0s 750us/step - loss: 0.1367 - accuracy: 0.9503\n",
      "Epoch 38/1500\n",
      "29/29 [==============================] - 0s 695us/step - loss: 0.1049 - accuracy: 0.9590\n",
      "Epoch 39/1500\n",
      "29/29 [==============================] - 0s 737us/step - loss: 0.1611 - accuracy: 0.9374\n",
      "Epoch 40/1500\n",
      "29/29 [==============================] - 0s 727us/step - loss: 0.1822 - accuracy: 0.9309\n",
      "Epoch 41/1500\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.1567 - accuracy: 0.9330\n",
      "Epoch 42/1500\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1503 - accuracy: 0.9438\n",
      "Epoch 43/1500\n",
      "29/29 [==============================] - 0s 795us/step - loss: 0.1284 - accuracy: 0.9503\n",
      "Epoch 44/1500\n",
      "29/29 [==============================] - 0s 687us/step - loss: 0.1035 - accuracy: 0.9590\n",
      "Epoch 45/1500\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.1339 - accuracy: 0.9460\n",
      "Epoch 46/1500\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.1334 - accuracy: 0.9438\n",
      "Epoch 47/1500\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.1557 - accuracy: 0.9374\n",
      "Epoch 48/1500\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.1637 - accuracy: 0.9482\n",
      "Epoch 49/1500\n",
      "29/29 [==============================] - 0s 670us/step - loss: 0.1506 - accuracy: 0.9503\n",
      "Epoch 50/1500\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1266 - accuracy: 0.9525\n",
      "Epoch 51/1500\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.1142 - accuracy: 0.9611\n",
      "Epoch 52/1500\n",
      "29/29 [==============================] - 0s 633us/step - loss: 0.1242 - accuracy: 0.9438\n",
      "Epoch 53/1500\n",
      "29/29 [==============================] - 0s 665us/step - loss: 0.1409 - accuracy: 0.9460\n",
      "Epoch 54/1500\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.1219 - accuracy: 0.9568\n",
      "Epoch 55/1500\n",
      "29/29 [==============================] - 0s 585us/step - loss: 0.1135 - accuracy: 0.9546\n",
      "Epoch 56/1500\n",
      "29/29 [==============================] - 0s 603us/step - loss: 0.1479 - accuracy: 0.9395\n",
      "Epoch 57/1500\n",
      "29/29 [==============================] - 0s 612us/step - loss: 0.1259 - accuracy: 0.9503\n",
      "Epoch 58/1500\n",
      "29/29 [==============================] - 0s 651us/step - loss: 0.1268 - accuracy: 0.9482\n",
      "Epoch 59/1500\n",
      "29/29 [==============================] - 0s 613us/step - loss: 0.1218 - accuracy: 0.9460\n",
      "Epoch 60/1500\n",
      "29/29 [==============================] - 0s 600us/step - loss: 0.1207 - accuracy: 0.9568\n",
      "Epoch 61/1500\n",
      "29/29 [==============================] - 0s 587us/step - loss: 0.1318 - accuracy: 0.9590\n",
      "Epoch 62/1500\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.1214 - accuracy: 0.9590\n",
      "Epoch 63/1500\n",
      "29/29 [==============================] - 0s 810us/step - loss: 0.1143 - accuracy: 0.9611\n",
      "Epoch 64/1500\n",
      "29/29 [==============================] - 0s 726us/step - loss: 0.1401 - accuracy: 0.9309\n",
      "Epoch 65/1500\n",
      "29/29 [==============================] - 0s 752us/step - loss: 0.1206 - accuracy: 0.9525\n",
      "Epoch 66/1500\n",
      "29/29 [==============================] - 0s 681us/step - loss: 0.1348 - accuracy: 0.9417\n",
      "Epoch 67/1500\n",
      "29/29 [==============================] - 0s 722us/step - loss: 0.1160 - accuracy: 0.9525\n",
      "Epoch 68/1500\n",
      "29/29 [==============================] - 0s 718us/step - loss: 0.1002 - accuracy: 0.9568\n",
      "Epoch 69/1500\n",
      "29/29 [==============================] - 0s 644us/step - loss: 0.1478 - accuracy: 0.9395\n",
      "Epoch 70/1500\n",
      "29/29 [==============================] - 0s 661us/step - loss: 0.1116 - accuracy: 0.9568\n",
      "Epoch 71/1500\n",
      "29/29 [==============================] - 0s 649us/step - loss: 0.1043 - accuracy: 0.9676\n",
      "Epoch 72/1500\n",
      "29/29 [==============================] - 0s 597us/step - loss: 0.0955 - accuracy: 0.9698\n",
      "Epoch 73/1500\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.1360 - accuracy: 0.9525\n",
      "Epoch 74/1500\n",
      "29/29 [==============================] - 0s 634us/step - loss: 0.1266 - accuracy: 0.9568\n",
      "Epoch 75/1500\n",
      "29/29 [==============================] - 0s 603us/step - loss: 0.1447 - accuracy: 0.9525\n",
      "Epoch 76/1500\n",
      "29/29 [==============================] - 0s 623us/step - loss: 0.1246 - accuracy: 0.9633\n",
      "Epoch 77/1500\n",
      "29/29 [==============================] - 0s 628us/step - loss: 0.1212 - accuracy: 0.9698\n",
      "Epoch 78/1500\n",
      "29/29 [==============================] - 0s 600us/step - loss: 0.1118 - accuracy: 0.9568\n",
      "Epoch 79/1500\n",
      "29/29 [==============================] - 0s 603us/step - loss: 0.1333 - accuracy: 0.9546\n",
      "Epoch 80/1500\n",
      "29/29 [==============================] - 0s 602us/step - loss: 0.1261 - accuracy: 0.9568\n",
      "Epoch 81/1500\n",
      "29/29 [==============================] - 0s 614us/step - loss: 0.0994 - accuracy: 0.9611\n",
      "Epoch 82/1500\n",
      "29/29 [==============================] - 0s 588us/step - loss: 0.1043 - accuracy: 0.9590\n",
      "Epoch 83/1500\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.1107 - accuracy: 0.9676\n",
      "Epoch 84/1500\n",
      "29/29 [==============================] - 0s 625us/step - loss: 0.1395 - accuracy: 0.9417\n",
      "Epoch 85/1500\n",
      "29/29 [==============================] - 0s 626us/step - loss: 0.0979 - accuracy: 0.9590\n",
      "Epoch 86/1500\n",
      "29/29 [==============================] - 0s 601us/step - loss: 0.1049 - accuracy: 0.9525\n",
      "Epoch 87/1500\n",
      "29/29 [==============================] - 0s 636us/step - loss: 0.1677 - accuracy: 0.9330\n",
      "Epoch 88/1500\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.1345 - accuracy: 0.9590\n",
      "Epoch 89/1500\n",
      "29/29 [==============================] - 0s 630us/step - loss: 0.1203 - accuracy: 0.9482\n",
      "Epoch 90/1500\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1288 - accuracy: 0.9438\n",
      "Epoch 91/1500\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.1214 - accuracy: 0.9590\n",
      "Epoch 92/1500\n",
      "29/29 [==============================] - 0s 609us/step - loss: 0.1395 - accuracy: 0.9438\n",
      "Epoch 93/1500\n",
      "29/29 [==============================] - 0s 612us/step - loss: 0.1230 - accuracy: 0.9438\n",
      "Epoch 94/1500\n",
      "29/29 [==============================] - 0s 623us/step - loss: 0.1002 - accuracy: 0.9611\n",
      "Epoch 95/1500\n",
      "29/29 [==============================] - 0s 586us/step - loss: 0.1024 - accuracy: 0.9590\n",
      "Epoch 96/1500\n",
      "29/29 [==============================] - 0s 624us/step - loss: 0.0934 - accuracy: 0.9546\n",
      "Epoch 97/1500\n",
      "29/29 [==============================] - 0s 607us/step - loss: 0.0680 - accuracy: 0.9741\n",
      "Epoch 98/1500\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.1209 - accuracy: 0.9633\n",
      "Epoch 99/1500\n",
      "29/29 [==============================] - 0s 604us/step - loss: 0.0959 - accuracy: 0.9546\n",
      "Epoch 100/1500\n",
      "29/29 [==============================] - 0s 617us/step - loss: 0.1149 - accuracy: 0.9568\n",
      "Epoch 101/1500\n",
      "29/29 [==============================] - 0s 610us/step - loss: 0.0994 - accuracy: 0.9633\n",
      "Epoch 102/1500\n",
      "29/29 [==============================] - 0s 697us/step - loss: 0.0948 - accuracy: 0.9698\n",
      "Epoch 103/1500\n",
      "29/29 [==============================] - 0s 637us/step - loss: 0.0937 - accuracy: 0.9633\n",
      "Epoch 104/1500\n",
      "29/29 [==============================] - 0s 673us/step - loss: 0.1769 - accuracy: 0.9374\n",
      "Epoch 105/1500\n",
      "29/29 [==============================] - 0s 639us/step - loss: 0.1608 - accuracy: 0.9568\n",
      "Epoch 106/1500\n",
      "29/29 [==============================] - 0s 658us/step - loss: 0.1033 - accuracy: 0.9611\n",
      "Epoch 107/1500\n",
      "29/29 [==============================] - 0s 656us/step - loss: 0.1329 - accuracy: 0.9525\n",
      "Epoch 108/1500\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.1674 - accuracy: 0.9417\n",
      "Epoch 109/1500\n",
      "29/29 [==============================] - 0s 726us/step - loss: 0.1247 - accuracy: 0.9633\n",
      "Epoch 110/1500\n",
      "29/29 [==============================] - 0s 650us/step - loss: 0.1059 - accuracy: 0.9633\n",
      "Epoch 111/1500\n",
      "29/29 [==============================] - 0s 667us/step - loss: 0.0602 - accuracy: 0.9784\n",
      "Epoch 112/1500\n",
      "29/29 [==============================] - 0s 645us/step - loss: 0.0946 - accuracy: 0.9741\n",
      "Epoch 113/1500\n",
      "29/29 [==============================] - 0s 659us/step - loss: 0.0930 - accuracy: 0.9611\n",
      "Epoch 114/1500\n",
      "29/29 [==============================] - 0s 699us/step - loss: 0.1042 - accuracy: 0.9719\n",
      "Epoch 115/1500\n",
      "29/29 [==============================] - 0s 721us/step - loss: 0.1423 - accuracy: 0.9482\n",
      "Epoch 116/1500\n",
      "29/29 [==============================] - 0s 657us/step - loss: 0.1427 - accuracy: 0.9482\n",
      "Epoch 117/1500\n",
      "29/29 [==============================] - 0s 640us/step - loss: 0.1020 - accuracy: 0.9590\n",
      "Epoch 118/1500\n",
      "29/29 [==============================] - 0s 605us/step - loss: 0.1215 - accuracy: 0.9633\n",
      "Epoch 119/1500\n",
      "29/29 [==============================] - 0s 597us/step - loss: 0.1675 - accuracy: 0.9352\n",
      "Epoch 120/1500\n",
      "29/29 [==============================] - 0s 676us/step - loss: 0.1158 - accuracy: 0.9654\n",
      "Epoch 121/1500\n",
      "29/29 [==============================] - 0s 683us/step - loss: 0.1172 - accuracy: 0.9568\n",
      "Epoch 122/1500\n",
      "29/29 [==============================] - 0s 675us/step - loss: 0.0931 - accuracy: 0.9654\n",
      "Epoch 123/1500\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.1093 - accuracy: 0.9482\n",
      "Epoch 124/1500\n",
      "29/29 [==============================] - 0s 682us/step - loss: 0.0921 - accuracy: 0.9633\n",
      "Epoch 125/1500\n",
      "29/29 [==============================] - 0s 975us/step - loss: 0.0942 - accuracy: 0.9676\n",
      "Epoch 126/1500\n",
      "29/29 [==============================] - 0s 707us/step - loss: 0.1229 - accuracy: 0.9503\n",
      "Epoch 127/1500\n",
      "29/29 [==============================] - 0s 679us/step - loss: 0.1336 - accuracy: 0.9568\n",
      "Epoch 128/1500\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.1243 - accuracy: 0.9568\n",
      "Epoch 129/1500\n",
      "29/29 [==============================] - 0s 652us/step - loss: 0.1044 - accuracy: 0.9719\n",
      "Epoch 130/1500\n",
      "29/29 [==============================] - 0s 662us/step - loss: 0.1160 - accuracy: 0.9654\n",
      "Epoch 131/1500\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.1256 - accuracy: 0.9654\n",
      "Epoch 132/1500\n",
      "29/29 [==============================] - 0s 666us/step - loss: 0.1143 - accuracy: 0.9525\n",
      "Epoch 133/1500\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0748 - accuracy: 0.9676\n",
      "Epoch 134/1500\n",
      "29/29 [==============================] - 0s 707us/step - loss: 0.1127 - accuracy: 0.9568\n",
      "Epoch 135/1500\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0828 - accuracy: 0.9762\n",
      "Epoch 136/1500\n",
      "29/29 [==============================] - 0s 655us/step - loss: 0.0771 - accuracy: 0.9719\n",
      "Epoch 137/1500\n",
      "29/29 [==============================] - 0s 692us/step - loss: 0.0786 - accuracy: 0.9719\n",
      "Epoch 138/1500\n",
      "29/29 [==============================] - 0s 691us/step - loss: 0.0767 - accuracy: 0.9762\n",
      "Epoch 139/1500\n",
      "29/29 [==============================] - 0s 672us/step - loss: 0.1044 - accuracy: 0.9633\n",
      "Epoch 140/1500\n",
      "29/29 [==============================] - 0s 678us/step - loss: 0.1094 - accuracy: 0.9590\n",
      "Epoch 141/1500\n",
      " 1/29 [>.............................] - ETA: 0s - loss: 0.0387 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 111.\n",
      "29/29 [==============================] - 0s 776us/step - loss: 0.1607 - accuracy: 0.9438\n",
      "Epoch 141: early stopping\n"
     ]
    }
   ],
   "source": [
    "optimizers = {\n",
    "    'Adamax': Adamax(learning_rate=0.003109800273709165)\n",
    "}\n",
    "\n",
    "# Full model definition with dynamic number of layers\n",
    "model_full = Sequential()\n",
    "model_full.add(Dense(128, activation='relu', input_shape=(X_train_full.shape[1],))) \n",
    "model_full.add(BatchNormalization())\n",
    "model_full.add(Dropout(0.44571035356880917))  \n",
    "model_full.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "\n",
    "# Compile the model\n",
    "model_full.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the full training set\n",
    "history_full = model_full.fit(X_train_full, y_train_full, epochs=1500, batch_size=16,\n",
    "                              verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82d1b1bc-abf1-4c4c-8f43-a7fa0e91f250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Encoding Verification:\n",
      "  age_group  encoded_label\n",
      "0    kitten            0.0\n",
      "1    senior            1.0\n"
     ]
    }
   ],
   "source": [
    "# verify encoded labels\n",
    "dataframe['encoded_label'] = y_encoded\n",
    "\n",
    "# Drop duplicates to find unique mappings\n",
    "unique_mappings = dataframe[['age_group', 'encoded_label']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Print the unique mappings for verification\n",
    "print(\"Class Encoding Verification:\")\n",
    "print(unique_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56241b4-a36b-4c64-9b34-57a93067cea5",
   "metadata": {},
   "source": [
    "# to do change this to demo set instead of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12694aaa-558e-4629-9eb7-ad580bad44e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Set Accuracy: 99.57%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on training set to get total accuracy\n",
    "loss, accuracy = model_full.evaluate(X_train_full, y_train_full, verbose=0)\n",
    "print(f\"Total Training Set Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6cfdaef-a0e2-4003-a756-c656bfe7d398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Set Accuracy: 99.57%\n",
      "Demo Set Accuracy: 92.86%\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Sample 0: Predicted=Senior, Actual=Senior, Score=0.9992\n",
      "Sample 1: Predicted=Kitten, Actual=Kitten, Score=0.1999\n",
      "Sample 2: Predicted=Senior, Actual=Kitten, Score=0.9878\n",
      "Sample 3: Predicted=Kitten, Actual=Kitten, Score=0.0675\n",
      "Sample 4: Predicted=Senior, Actual=Senior, Score=1.0000\n",
      "Sample 5: Predicted=Kitten, Actual=Kitten, Score=0.3152\n",
      "Sample 6: Predicted=Senior, Actual=Senior, Score=0.9998\n",
      "Sample 7: Predicted=Kitten, Actual=Kitten, Score=0.0923\n",
      "Sample 8: Predicted=Kitten, Actual=Kitten, Score=0.0546\n",
      "Sample 9: Predicted=Senior, Actual=Senior, Score=0.9738\n",
      "Sample 10: Predicted=Senior, Actual=Senior, Score=0.9995\n",
      "Sample 11: Predicted=Senior, Actual=Senior, Score=0.9924\n",
      "Sample 12: Predicted=Kitten, Actual=Kitten, Score=0.0839\n",
      "Sample 13: Predicted=Senior, Actual=Senior, Score=0.9994\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set to get total accuracy\n",
    "loss, accuracy = model_full.evaluate(X_train_full, y_train_full, verbose=0)\n",
    "print(f\"Total Training Set Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the demo set to get accuracy\n",
    "loss, accuracy = model_full.evaluate(X_demo, y_demo, verbose=0)\n",
    "print(f\"Demo Set Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict probabilities for the demo samples\n",
    "probabilities = model_full.predict(X_demo)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "predictions = (probabilities > 0.5).astype(int)\n",
    "\n",
    "# Map predictions and actual labels to \"Kitten\" or \"Senior\"\n",
    "label_map = {0: 'Kitten', 1: 'Senior'}\n",
    "mapped_predictions = [label_map[pred[0]] for pred in predictions]\n",
    "mapped_actual_labels = [label_map[int(label)] for label in y_demo]\n",
    "\n",
    "# Print out the probabilities along with actual labels and predictions\n",
    "for i in range(len(probabilities)):\n",
    "    print(f\"Sample {i}: Predicted={mapped_predictions[i]}, Actual={mapped_actual_labels[i]}, Score={probabilities[i][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79691c41-3a64-470d-8c8f-51f440963ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step\n",
      "Sample 0: Probability=0.9992295503616333, Actual Label=1.0\n",
      "Sample 1: Probability=0.19993403553962708, Actual Label=0.0\n",
      "Sample 2: Probability=0.9877561330795288, Actual Label=0.0\n",
      "Sample 3: Probability=0.06748417019844055, Actual Label=0.0\n",
      "Sample 4: Probability=0.9999938011169434, Actual Label=1.0\n",
      "Sample 5: Probability=0.31522753834724426, Actual Label=0.0\n",
      "Sample 6: Probability=0.9997661709785461, Actual Label=1.0\n",
      "Sample 7: Probability=0.09229150414466858, Actual Label=0.0\n",
      "Sample 8: Probability=0.05456143617630005, Actual Label=0.0\n",
      "Sample 9: Probability=0.9737595915794373, Actual Label=1.0\n",
      "Sample 10: Probability=0.999523401260376, Actual Label=1.0\n",
      "Sample 11: Probability=0.9924240112304688, Actual Label=1.0\n",
      "Sample 12: Probability=0.08393140882253647, Actual Label=0.0\n",
      "Sample 13: Probability=0.9993523955345154, Actual Label=1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for the demonstration samples\n",
    "probabilities = model_full.predict(X_demo)\n",
    "\n",
    "# Print out the probabilities along with actual labels\n",
    "for i in range(len(probabilities)):\n",
    "    print(f\"Sample {i}: Probability={probabilities[i][0]}, Actual Label={y_demo[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bf63c-14a7-4474-b195-3c12a4ee3813",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af34fc89-a102-40da-96c9-770f30406f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the StandardScaler\n",
    "joblib.dump(scaler_full, 'scaler_full.pkl')\n",
    "\n",
    "# Save the trained model\n",
    "model_full.save('cat_age_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c47fb-ea5a-4752-b187-e30e19af2c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
